#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç - –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ETL —Å–∏—Å—Ç–µ–º—ã
"""

import sys
from pathlib import Path
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é
sys.path.append(str(Path(__file__).parent.parent))

from app.models.metadata import DataSource, Pipeline, PipelineRun, DataSourceType, PipelineStatus, PipelineRunStatus
from app.models.staging import FileMetadata, FileStatus
from app.connectors.database_manager import db_manager


def create_sample_data():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ë–î
    session = db_manager.get_metadata_session()
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö
        data_source = DataSource(
            name="CSV —Ñ–∞–π–ª –ø—Ä–æ–¥–∞–∂",
            description="–ï–∂–µ–º–µ—Å—è—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–¥–∞–∂ –≤ CSV —Ñ–æ—Ä–º–∞—Ç–µ",
            source_type="file",
            connection_config={
                "file_path": "/data/sales_2024.csv",
                "delimiter": ",",
                "encoding": "utf-8",
                "has_header": True
            },
            is_active=True
        )
        session.add(data_source)
        session.flush()  # –ü–æ–ª—É—á–∞–µ–º ID
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
        pipeline = Pipeline(
            name="–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–∞–∂",
            description="–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂",
            data_source_id=data_source.id,
            status="active",
            configuration={
                "validation_rules": {
                    "required_columns": ["date", "amount", "product_id", "customer_id"],
                    "data_types": {
                        "date": "date",
                        "amount": "float",
                        "product_id": "integer",
                        "customer_id": "integer"
                    },
                    "constraints": {
                        "amount": {"min": 0},
                        "date": {"format": "%Y-%m-%d"}
                    }
                },
                "transformations": [
                    "clean_duplicates",
                    "validate_amounts", 
                    "enrich_product_data",
                    "calculate_totals"
                ],
                "output_config": {
                    "target_table": "sales_final",
                    "partition_by": "date"
                }
            },
            schedule_cron="0 2 * * *",  # –ö–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ 2:00
            max_retries=3,
            timeout_seconds=3600,
            tags=["sales", "monthly", "critical"]
        )
        session.add(pipeline)
        session.flush()
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
        pipeline_run = PipelineRun(
            pipeline_id=pipeline.id,
            status="success",
            started_at=datetime.now(),
            finished_at=datetime.now(),
            duration_seconds=120,
            records_processed=10000,
            records_failed=0,
            triggered_by="manual_test",
            retry_count=0
        )
        session.add(pipeline_run)
        session.flush()
        
        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        from app.models.metadata import AnalysisResult
        
        analysis_result = AnalysisResult(
            pipeline_run_id=pipeline_run.id,
            analysis_type="data_quality",
            result_data={
                "completeness": 99.8,
                "accuracy": 98.5,
                "consistency": 97.2,
                "validity": 99.1
            },
            metrics={
                "null_percentage": 0.2,
                "duplicate_percentage": 0.1,
                "outlier_percentage": 1.5
            },
            recommendations=[
                "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü –Ω–∞ –ø—Ä–µ–¥–º–µ—Ç –∞–Ω–æ–º–∞–ª–∏–π",
                "–î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è –ø–æ–ª–µ–π customer_id",
                "–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–ª–µ—Ä—Ç—ã –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏ –ø–æ—Ä–æ–≥–∞ outliers > 2%"
            ],
            is_alert=False,
            alert_level=None
        )
        session.add(analysis_result)
        
        session.commit()
        
        print("‚úÖ –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã:")
        print(f"   ‚Ä¢ –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–∞–Ω–Ω—ã—Ö: {data_source.name} (ID: {data_source.id})")
        print(f"   ‚Ä¢ –ü–∞–π–ø–ª–∞–π–Ω: {pipeline.name} (ID: {pipeline.id})")
        print(f"   ‚Ä¢ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞: ID {pipeline_run.id}")
        print(f"   ‚Ä¢ –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞: ID {analysis_result.id}")
        
    except Exception as e:
        session.rollback()
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise
    finally:
        session.close()


def create_sample_file():
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ —Ñ–∞–π–ª–∞ –≤ —Ä–∞–±–æ—á–µ–π –ë–î"""
    
    session = db_manager.get_staging_session()
    
    try:
        # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
        file_metadata = FileMetadata(
            filename="sales_2024_01.csv",
            original_filename="sales_2024_01.csv",
            file_path="/data/upload/sales_2024_01.csv",
            file_size=1024000,  # 1MB
            file_hash="a1b2c3d4e5f6789012345678901234567890abcdef1234567890abcdef123456",
            mime_type="text/csv",
            encoding="utf-8",
            status="processed",
            uploaded_by="admin",
            processing_started_at=datetime.now(),
            processing_finished_at=datetime.now(),
            metadata={
                "columns": ["date", "amount", "product_id", "customer_id"],
                "row_count": 10000,
                "delimiter": ",",
                "has_header": True
            }
        )
        session.add(file_metadata)
        session.commit()
        
        print(f"‚úÖ –§–∞–π–ª —Å–æ–∑–¥–∞–Ω: {file_metadata.filename} (ID: {file_metadata.id})")
        
    except Exception as e:
        session.rollback()
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
        raise
    finally:
        session.close()


def show_sample_queries():
    """–ü–æ–∫–∞–∑–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    print("\nüìä –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:")
    print("=" * 50)
    
    # –ó–∞–ø—Ä–æ—Å –∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º –ë–î
    try:
        with db_manager.get_metadata_session() as session:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø–∞–π–ø–ª–∞–π–Ω—ã
            pipelines = session.execute("""
                SELECT p.name, p.status, ds.name as source_name, p.created_at
                FROM pipelines p
                JOIN data_sources ds ON p.data_source_id = ds.id
                ORDER BY p.created_at DESC
            """)
            
            print("\nüóÑÔ∏è  –ü–∞–π–ø–ª–∞–π–Ω—ã:")
            for pipeline in pipelines:
                print(f"   ‚Ä¢ {pipeline[0]} ({pipeline[1]}) - –∏—Å—Ç–æ—á–Ω–∏–∫: {pipeline[2]}")
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø—É—Å–∫–∏
            runs = session.execute("""
                SELECT p.name, pr.status, pr.records_processed, pr.duration_seconds
                FROM pipeline_runs pr
                JOIN pipelines p ON pr.pipeline_id = p.id
                ORDER BY pr.created_at DESC
                LIMIT 5
            """)
            
            print("\nüîÑ –ü–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø—É—Å–∫–∏:")
            for run in runs:
                print(f"   ‚Ä¢ {run[0]}: {run[1]}, –∑–∞–ø–∏—Å–µ–π: {run[2]}, –≤—Ä–µ–º—è: {run[3]}—Å")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º: {e}")
    
    # –ó–∞–ø—Ä–æ—Å –∫ —Ä–∞–±–æ—á–µ–π –ë–î
    try:
        with db_manager.get_staging_session() as session:
            files = session.execute("""
                SELECT filename, status, file_size, created_at
                FROM file_metadata
                ORDER BY created_at DESC
                LIMIT 5
            """)
            
            print("\nüìÅ –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Ñ–∞–π–ª—ã:")
            for file in files:
                print(f"   ‚Ä¢ {file[0]} ({file[1]}), —Ä–∞–∑–º–µ—Ä: {file[2]} –±–∞–π—Ç")
                
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ —Ä–∞–±–æ—á–µ–π –ë–î: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç ETL —Å–∏—Å—Ç–µ–º—ã")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π...")
    results = db_manager.test_connections()
    
    if not all(results.values()):
        print("‚ùå –ù–µ –≤—Å–µ –ë–î –¥–æ—Å—Ç—É–ø–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã:")
        print("   docker-compose up -d metadata-postgres staging-postgres target-clickhouse")
        return
    
    print("‚úÖ –í—Å–µ –ë–î –¥–æ—Å—Ç—É–ø–Ω—ã")
    
    # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("\nüìù –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    try:
        create_sample_data()
        create_sample_file()
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤
        show_sample_queries()
        
        print("\nüéâ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç –∑–∞–≤–µ—Ä—à–µ–Ω!")
        print("\nüìö –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("   1. –ò–∑—É—á–∏—Ç–µ README_ETL_DATABASE.md –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏")
        print("   2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ python scripts/db_cli.py –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ë–î")
        print("   3. –°–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ–∏ –ø–∞–π–ø–ª–∞–π–Ω—ã —á–µ—Ä–µ–∑ API –∏–ª–∏ –Ω–∞–ø—Ä—è–º—É—é –≤ –ë–î")
        print("   4. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –∞–ª–µ—Ä—Ç—ã")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        print("\nüí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
        print("   1. –í—Å–µ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã –∑–∞–ø—É—â–µ–Ω—ã")
        print("   2. –ú–∏–≥—Ä–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã: python scripts/migrate.py all upgrade head")
        print("   3. ClickHouse –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: python scripts/init_clickhouse.py")


if __name__ == "__main__":
    main()
